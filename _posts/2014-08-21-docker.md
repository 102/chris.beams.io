---
layout: post
title: Ideas On a P2P Docker Cloud
date: Thu Aug 21 23:59:00 CEST 2014
timestamp: 19007097
---

_Docker and its useful metaphor of application shipping containers are already a big deal, but I think much of their potential is yet to be realized. To begin exploring that potential, I'll containerize this site over the next few days and chronicle the process._

If you're a developer, you're probably already familiar with [Docker](https://www.docker.com/) to at least some degree, so I'll keep the introduction brief. Essentially, by defining a commonly agreed-upon software container for applications of all kinds, Docker hopes to do for software development in the 21st century what [intermodal shipping containers](https://en.wikipedia.org/wiki/Intermodal_container) did for freight transport in the 20th.

They're having quite a bit of success at it too. For example, a quick search for [Docker Meetups](http://www.meetup.com/find/?%2Ffind%2F=undefined&allMeetups=false&keywords=Docker&radius=Infinity&userFreeform=Vienna%2C+Austria&mcName=Vienna%2C+AT&lat=48.2166&lon=16.3927&sort=default) reveals that most major cities have one—always a good sign. As another positive indicator, I subscribe to Docker's newsletter and every week there are dozens of links to independent blog posts and articles extolling Docker's virtues. Like shipping containers, Docker's utility increases directly with the number of people and organizations that choose to use it, and it's clear that a great many have done so.

The container that Docker offers is based on [Linux containers](https://en.wikipedia.org/wiki/LXC) (LXCs), which provide many of the same benefits of traditional virtualization (isolation of resources, etc), without actually forcing users to deal with a hypervisor. It all feels much lighter-weight because, well, _it is_.

LXCs have been around for a while, and they are in turn based on previous work with _chroot jails_ and similar technologies; so this is nothing new per se. What Docker adds to the mix are developer and sysadmin-friendly tools and resources for composing, sharing and running LXCs in such a way as to make them highly portable and able to be managed in a consistent fashion.

If you're interested in learning more, I recommend walking through Docker's excellent [user guide](https://docs.docker.com/userguide/). I did so a few weeks ago; it takes just a couple hours and is well worth the time.

Most of the interest in Docker is coming from startups and forward-leaning enterprises who want to use it to make their DevOps arrangements ever more efficient. In response,  [PaaS](https://en.wikipedia.org/wiki/Platform_as_a_service) providers like Cloud Foundry, Elastic Beanstalk, and Google App Engine [are](http://blog.pivotal.io/cloud-foundry-pivotal/products/managing-stateful-docker-containers-with-cloud-foundry-bosh) [already](https://aws.amazon.com/blogs/aws/aws-elastic-beanstalk-for-docker/) [supporting](http://blog.docker.com/2014/07/dockercon-video-docker-on-google-app-engine/) Docker containers to some degree, and more are sure to follow.

Meanwhile, others are proposing that Docker may change the PaaS landscape in more fundamental ways. Rather than retrofitting support for containers, new PaaS offerings like [Deis](http://deis.io) are being built from the ground up on Docker. This container-first approach has been called (by at least [one article](http://thenewstack.io/docker-is-driving-a-new-breed-of-paas/)) "the third wave of PaaS".

Then there are companies like [Tutum](https://www.tutum.co/) who don't advertise themselves as a PaaS, but rather as "CaaS": _Container as a Service_. This makes sense because if absolutely everything an application might need is packaged within a container somewhere, then all you need is infrastructure that helps you stack and connect the right containers with each other.

This is all well and good, but I think that the trend toward CaaS can be taken even further. Why not build a fully peer-to-peer CaaS? Why not establish a marketplace in which regular individuals can sell their unused computing resources to host other users' containerized applications?

A great many things would have to be worked out. Reliability, redundancy, backups, load balancing, scaling, DNS, etc. There is a reason that only a handful of companies in the world offer public PaaS solutions, and that reason is because it's intensely difficult to get right. An internet-scale peer-to-peer cloud for container hosting would be in many ways even harder.

But one thing makes me think that it's worth considering: bitcoin. With bitcoin, and particularly bitcoin [micropayment channels](https://bitcoin.org/en/developer-guide#micropayment-channel), we now have the opportunity to pay in real time for metered use of computing resources. No monthly billing, no [coarse-grained pricing plans](https://www.tutum.co/pricing/), just paying for exactly what you use, for exactly as long as you use it, with prices determined by a free market of competing providers and users.

I'm being criminally brief and creating more questions than answers, I know. But rather than go on at length in idle speculation, I'd prefer to get busy containerizing this site, and then see just how far one can go in attaching such "meters" to a Docker container. If one can measure precisely how much bandwidth, cpu, memory and disk a container is using at any given moment, then one can bill for it accordingly. Assuming I get that far, I'll then try to mock this up using [BitcoinJ's micropayment channel support](https://bitcoinj.github.io/working-with-micropayments), and report back what I find along the way.

Another important aspect of such a peer-to-peer container hosting cloud would be ensuring the opacity of a container's contents. Ideally, the party hosting a given container should have no ability to know what's inside the box. For a simple example, imagine that the container you're hosting contains someone's email server. They certainly don't want you to have access to the contents, and you as a hosting provider should want as much plausible deniability as you can get. So far as I've researched, Docker provides no support for these kind of "black box containers", and I don't yet know enough about Docker or the validity of this idea in general to begin asking for it. It's something I'll be thinking about along the way, though.

One thing I should mention in closing is that Bitcoin Core developer Jeff Garzik sketched a similar idea earlier this year on his blog—he even dubbed it "[blackbox](http://garzikrants.blogspot.co.at/2014/01/blackbox-bitcoin-enabled-decentralized.html)". I arrived indepentently at the idea I've described above, but was heartened to stumble on Jeff's post during subsequent research.

I'll also mention that there are a number of other projects going on in the larger bitcoin space that aim to create massive peer-to-peer compute and storage clouds. A couple of the better known efforts are [Maidsafe](http://maidsafe.net/) and [Storj](http://storj.io/). These projects are interesting and I wish them well. Both projects, however, require users to do things in completely different ways—to use their APIs and client software and so on. They may be successful in the long run, but in the meantime, I think a great deal of progress could be quickly made by using off the shelf-componentry like Docker in combination with a [DHT](https://duckduckgo.com/Distributed_hash_table)-based p2p marketplace, metered resource usage and instantaneous settlement via bitcoin micropayment channels.

We shall see. I'll be back down to earth tomorrow with a post on my first steps in containerizing this simple little site. First things first, and all.

